---
title: "Johns Hopkins Capstone W02"
author: "shostiou"
date: "24/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document is used as a base to the Johns Hopkins final Capstone project from the Data Science specialization. The purpose of the project is to apply data science skills to Natural Language Processing (NLP) / Text mining problematics.  
The current document is the intermediate report which is due as an assignment for week 02.  
<br/>

**References**  
As suggested by the instructors, my own researches where done to get a basic understanding of Text Mining and Natural Language Processing.  
The book "Text Mining with R" (https://www.tidytextmining.com/) was a great source of inspiration to define my approach.  
The tidytext package which can be used in conjunction with dplyr will be used as a core component to manipulate text.  

```{r, echo = FALSE,message=FALSE,warning=FALSE}
library(tidyverse)
library(ggplot2)
library(tidytext)
library(readtext)
library(corpus)
library(R.utils)
```

<br/>
**Sampling of the text files**  
The 3 text files of English language will be uploaded into a R dataframe (Tibble).  
In order to limit memory consumption and to improve performance, a subset of the 3 files will be built by applying random sampling (seeds set to a constant value for reproducibility). Sample size will be limited to 5% of the original data sets. Please note that for the convenience of the reader, code related to reading & sampling operations of the file has been hidden.

```{r, echo=FALSE}
# Building the file directory  
file_dir <- getwd()
file_dir <- paste0(file_dir,'/data/final/en_US/')
# files name to open
file_name <- c('en_US.blogs.txt','en_US.news.txt','en_US.twitter.txt')
file_path = paste0(file_dir,file_name)

```


```{r, echo=FALSE,message=FALSE,warning=FALSE, cache=TRUE}

# Setting the seeds
set.seed(05091810)
# Initiating Empty variables
raw_text <- ''
en_US_tibble <- tibble(dataset=character(),text=character())

# Looping on the list of files
for (f_name in file_name){

  # Temporary Tibble  
  temp_tibble <- tibble(dataset=character(),text=character())
  
  # file path  
  file_path = paste0(file_dir,f_name)
  # Counting the lines of the raw text
  raw_text_lines <- countLines(file_path)
  # Creating an index of samples
  sample_of_lines <- sample(seq(1,raw_text_lines,by = 1), 
                            size = round(raw_text_lines/20), replace = F)

  # Adding to the tibble
  # A regex is used to remove the file extension. 
  temp_tibble <- tibble(dataset=sub('\\.txt$', '', f_name),
                        text=(readLines(file_path)[sample_of_lines]))
  en_US_tibble <- rbind(en_US_tibble,temp_tibble)

} 

# Freeing memory by suppressing unnecessary variables
rm(temp_tibble)


```


## Summary & Tokenization

The table below summarizes the number of lines contained in each sample extracted from the text files.

```{r, echo=FALSE, message = FALSE, warning=FALSE, comment=""}
en_US_tibble %>%
  group_by(dataset) %>%
  count(text) %>% summarise(nb_lines = sum(n))
```

<br/>
**Tokenization**  
Tokenization will be done by using the unnest_tokens function of the tidytext package. 
In order to compare word frequencies between the 3 datasets (blogs, news, twitter), a column will be kept to keep track of the source files.

```{r, echo=FALSE}
en_US_token <- en_US_tibble %>% unnest_tokens(word,text)
# Defining  the frequency of words for each file
en_US_token_by_file <- en_US_token %>% group_by(dataset) %>% count(word) %>% mutate(freq_word = n/sum(n)) %>% arrange(dataset,desc(freq_word))
# Switching the dataset column (source file) to factor
en_US_token_by_file$dataset <- as.factor(en_US_token_by_file$dataset)

```

As a result of tokenization, the number of words contained in each of the 3 samples is given in the table below. We can observe that the number of samples of words is correctly balanced between the 3 data sets.  


```{r, echo=FALSE, warning=FALSE,message=FALSE, comment=""}
en_US_token %>% group_by(dataset) %>% count(word) %>% summarize(nb_of_words = sum(n))
```

<br/>

**Comment about removal of "stop words"**  
Based on my readings, a common practive in text mining, is to remove "stop words" from the datasets.  
As the purpose of this project is to predict from a first word what could be the next 2 or 3 words, it seems essential to keep those stop words as part of the dataset as they appear to be required to create explicit transitions with other words.  
Removing stop words is essential while dealing with content analysis of a text but this is not the purpose of this project.
<br/>


**Markov chain**  

https://rpubs.com/Argaadya/markov-chain

```{r}

```



## EDA - Frequency of single words  

As a first step, we can compare which are the most frequent words in the blogs, news and twitter datasets. The plots below identify the words with a frequency superior to 0.05%.  
We can observe that from a general perspective, the 3 text files seem to share the same set of most frequent words. Those words appear to be "stop words" for most of them.  


```{r, echo=FALSE}
en_US_token_by_file %>% filter(freq_word > 0.005) %>% ggplot()+
  geom_col(aes(x=reorder(word,freq_word),y=freq_word)) +
  coord_flip()+
  facet_grid(. ~ dataset)+
  ylab('Frequency of words')+
  xlab('Most frequent words')+
  ggtitle('Most frequent words')
```

<br/>


### Exploring the combined data set  

As part of a preliminary step, the frequency of words was compared by distinguishing the 3 data sets. In this step, the exploration will be focused on a single data set resulting of a combination of "blogs", "news" and "twitter" data.  

```{r, echo=FALSE}
en_US_token_com <- en_US_token %>% select(-dataset) %>% count(word) %>% mutate(freq_word = n/sum(n)) %>% arrange(desc(freq_word))
```

We can observe, and this is not a surprise, that the most frequent words are stop words. This underlines the importance of keeping those words as part of our training set because they will probably play a key role in our prediction.
Seen from another perspective, we can visualize this prevalence of words thanks to a cloud of words graph.  

```{r,echo=FALSE, warning=FALSE, message=FALSE}
library(wordcloud)
library(RColorBrewer)
en_US_token_com %>% with(wordcloud(word, freq_word, max.words = 150))
```
<br/>

## Relationships between 2 & 3 words  

In this step we will identify which words tend to follow each others. As a first exploratory approach, we will identify the pairs of adjacent words (which are called "bi-grams"). The most frequent bigrams appears to be the ones linking the common stop words. This is not a surprise because couples like "of the", "in the" are fundamental elements of transitions of the English language. They need to be correctly implemented in our text prediction model.  
The analysis can be extended to trigrams (relationship between 3 consecutive words).  
The relationships which are appearing from this tokenization are strong and will probably result in making good predictions once embedded into the model. The graphs below illustrate what are the most frequent bigrams and trigrams used in the english american language.
<br/>

```{r, echo=FALSE, cache=TRUE}
en_US_bigram <- en_US_tibble %>% 
  unnest_tokens(bigram,text, token = "ngrams",n=2) %>%
  count(bigram, sort = TRUE) %>%
  mutate(freq_bigram = n/sum(n))
  
```

```{r, echo=FALSE}
plot1 <- en_US_bigram %>% filter(freq_bigram > 0.0008) %>% 
  mutate(bigram = reorder(bigram, freq_bigram)) %>%
  ggplot() +
  geom_col(aes(x=bigram,y=freq_bigram), fill='darkgreen')+
  coord_flip()+
  xlab('bigrams')+
  ylab('frequency of bigrams')+
  ggtitle('Frequency of bigrams')
```

```{r, echo=FALSE, cache=TRUE}
en_US_trigram <- en_US_tibble %>% 
  unnest_tokens(trigram,text, token = "ngrams",n=3) %>%
  count(trigram, sort = TRUE) %>%
  mutate(freq_trigram = n/sum(n)) %>%
  na.omit()
  
```

```{r, echo=FALSE}
plot2 <- en_US_trigram %>% na.omit() %>% filter(freq_trigram > 0.0001) %>% 
  mutate(trigram = reorder(trigram, freq_trigram)) %>%
  ggplot() +
  geom_col(aes(x=trigram,y=freq_trigram), fill='darkred')+
  coord_flip()+
  xlab('trigrams')+
  ylab('frequency of trigrams')+
  ggtitle('Frequency of trigrams')
```


```{r, echo=FALSE}
library(cowplot)
plot_grid(plot1, plot2, labels = "AUTO")

```

## Distributions of words and n-grams  

The last step of exploratory data analysis will be focused on the examination of the distribution of the usage of single words / bigrams / trigrams.  
The target of this EDA is to identify if a cutoff can be identify to exclude irrelevant words of bigrams / trigrams from the datasets.  
We can observe that the distributions are extremely right skewed. The vast majority of words, bigrams or trigrams have a limited number of occurrence in the data set. In order to avoid losing relevant information, I took the decision not to exclude any information from the dataset.


```{r, echo=FALSE}
plot1 <- en_US_token_com %>% filter(n>0,n<40) %>%
  ggplot()+
  geom_histogram(mapping = aes((n)))+
  ggtitle('Distribution of single words')
plot2 <- en_US_bigram %>% filter(n>0,n<40) %>%
  ggplot()+
  geom_histogram(mapping = aes((n)))+
  ggtitle('Distribution of bigrams')
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(cowplot)
plot_grid(plot1, plot2, labels = "AUTO")

```



## Plans for Model & Shiny App definition  

My intention is to use the bigrams and trigrams identified during this data exploration as the key elements of next word prediction.  
The Shiny application defined for this project will offer an input interface which will invite the user to type some text. The app will automatically identify which words are typed by detecting white spaces ' ' between the strings.  

- If the user has typed only one word => based on frequency analysis of the bigrams corresponding to this word, the model will display a suggestion of the three most likely words.  
- If the user has typed more than one word => The system will detect which are the 2 last words typed by the user. Based on frequency analysis of the trigrams corresponding to those 2 words, the model will display a suggestion of the three most likely words.  
<br/>
The suggestions will be displayed in a way to show the most likeky word first.  



```{r, echo=FALSE}
save(en_US_token,file="en_US_token.Rda")
save(en_US_bigram,file="en_US_bigram.Rda")
save(en_US_trigram,file="en_US_trigram.Rda")
```




