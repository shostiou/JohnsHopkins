---
title: "JH_capstone_W01"
author: "shostiou"
date: "17/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document is used as a base to the Johns Hopkins final Capstone project from the Data Science specialization.  
The purpose of the project is to apply data science skills to Natural Language Processing (NLP) / Text mining problematics.

## Basic requirements

As a preliminary step, let's start by calling the packages which will be required for the project.  
As suggested by the instructors, I spent some time to get a basic understanding of what NLP / text mining is.  
As I was looking for a "hands on" approach, I rapidly discovered that a set of packages where available to address such kind of problems.  
Tidytext is one of those important packages in R.  
The readtext package will alos be required to be able to open a text file.  
The text mining library tm appears alos to be a must.

```{r, message=FALSE,warning=FALSE}
library(tidyverse)
library(ggplot2)
library(tidytext)
library(readtext)
library(corpus)
library(R.utils)
```


## Basic approach  

### Reading the text file   

The text file will be upload into a R dataframe.  

```{r}
# Building the file directory  
file_dir <- getwd()
file_dir <- paste0(file_dir,'/data/final/en_US/')
# files name to open
file_name <- c('en_US.twitter.txt','en_US.news.txt','en_US.twitter.txt')
file_path = paste0(file_dir,file_name)

```

Opening the file with the readtext command readLines
Then, we create a subsample to limit the size of the data in memory
The data will be stored into a dataframe (tibble) made of 2 columns :  
- dataset = file name used as a source to collecte the data  
- text = content of the text line read from the txt files.


```{r, message=FALSE,warning=FALSE}

# Setting the seeds
set.seed(05091810)
# Initiating Empty variables
raw_text <- ''
en_US_tibble <- tibble(dataset=character(),text=character())

# Looping on the list of files
for (f_name in file_name){
  print(f_name)
  
  # Temporary Tibble  
  temp_tibble <- tibble(dataset=character(),text=character())
  
  # file path  
  file_path = paste0(file_dir,f_name)
  # Counting the lines of the raw text
  raw_text_lines <- countLines(file_path)
  # Creating an index of samples
  sample_of_lines <- sample(seq(1,raw_text_lines,by = 1), 
                            size = round(raw_text_lines/100), replace = F)
  # Reading the samples
  #temp_tibble$text <- tibble(readLines(file_path)[sample_of_lines]
  # Adding to the tibble
  # A regex is used to remove the file extension. 
  temp_tibble <- tibble(dataset=sub('\\.txt$', '', f_name),
                        text=(readLines(file_path)[sample_of_lines]))
  en_US_tibble <- rbind(en_US_tibble,temp_tibble)

} 

# Freeing memory by suppressing unnecessary variables
rm(temp_tibble)


```


## Tokenization

This first step will be carried with the support of the unnest_tokens function which is part of the tidy_text package.  
As I am interested in comparing the word frequencies between the 3 files used as a source, the dataset column of the dataframe will be kept in this first step.

```{r}
en_US_token <- en_US_tibble %>% unnest_tokens(word,text)
# Defining  the frequency of words for each file
en_US_token_by_file <- en_US_token %>% group_by(dataset) %>% count(word) %>% mutate(freq_word = n/sum(n)) %>% arrange(dataset,desc(freq_word))
```

According to my readings, a common usage in text mining, is to remove "stop words" from the datasets.  
As the purpose of this project is to predict what could be the next 2 or 3 words, it seems essential to keep those stop words into the dataset as they appear to be essential to created transitions with other words.  







Turning the text to a tibble

```{r}
twitter_us_df <- tibble(text=raw_text_list)
```

```{r}
twitter_us_df %>%  
  unnest_tokens(word,text) %>%
  count(word) %>%
  arrange(desc(n))
  
```

A common practice there would be to remove the stop words. For the purpose of our application, those words will be kept.  









prints the maximum length of a line of text from the string vector  

```{r}
max(nchar(raw_text))
```

For the twitter dataset = 140 (normal)  
For the news dataset = 11384  
For the blog 40833.  

Finding the number of occurences of words love an hate in the Twitter dataset.  

```{r}
nb_love <- sum(str_count(raw_text, "love"))
nb_have <- sum(str_count(raw_text, "hate"))
```

Finding the occurence of Biostats

```{r}
raw_text[str_detect(raw_text, "biostats")] 
```

A computer once beat me at chess, but it was no match for me at kickboxing

```{r}
raw_text[str_detect(raw_text, "A computer once beat me at chess, but it was no match for me at kickboxing")] 
```


## Basic requirements

```{r}
raw_text_df %>%  
  unnest_tokens(word,text) %>%
  count(word) %>%
  arrange(desc(n))
  
```

## Sampling

Due to the large size of the datasets, they are extremley painful to manipulate.  
EDA will be built on a subset (10%) of randomly sampled datasets.  

```{r}
set.seed(05091810)
```

```{r}
# Create a sample of 50 numbers which are incremented by 1.
x <- seq(0,50,by = 1)

sample(x,size=3, replace =F)
```


