---
title: "JH_capstone_W02"
author: "shostiou"
date: "17/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document is used as a base to the Johns Hopkins final Capstone project from the Data Science specialization.  
The purpose of the project is to apply data science skills to Natural Language Processing (NLP) / Text mining problematics.  
The current document is the intermediate report which is due as an assignment for week 02.  
### References  

As suggested by the instructors, my own researches where done to get a basic understanding of Text Mining and Natural Language Processing.  
The book "Text Mining with R" (https://www.tidytextmining.com/) was a great source of inspiration to define my approach.


### Packages

The tidytext package which can be used in conjunction with dplyr will be used as a core component to manipulate text.


```{r, message=FALSE,warning=FALSE}
library(tidyverse)
library(ggplot2)
library(tidytext)
library(readtext)
library(corpus)
library(R.utils)
```


## Basic approach  

### Reading the text file   

The 3 text files of English language will be uploaded into a R dataframe (Tibble).  
In order to limit memory consumption and to improve performance, a subset of the 3 files will be built by applying random sampling.

```{r}
# Building the file directory  
file_dir <- getwd()
file_dir <- paste0(file_dir,'/data/final/en_US/')
# files name to open
file_name <- c('en_US.blogs.txt','en_US.news.txt','en_US.twitter.txt')
file_path = paste0(file_dir,file_name)

```

This chunck will loop through the content of the 3 files and build a data frame.

```{r, message=FALSE,warning=FALSE}

# Setting the seeds
set.seed(05091810)
# Initiating Empty variables
raw_text <- ''
en_US_tibble <- tibble(dataset=character(),text=character())

# Looping on the list of files
for (f_name in file_name){
  print(f_name)
  
  # Temporary Tibble  
  temp_tibble <- tibble(dataset=character(),text=character())
  
  # file path  
  file_path = paste0(file_dir,f_name)
  # Counting the lines of the raw text
  raw_text_lines <- countLines(file_path)
  # Creating an index of samples
  sample_of_lines <- sample(seq(1,raw_text_lines,by = 1), 
                            size = round(raw_text_lines/100), replace = F)

  # Adding to the tibble
  # A regex is used to remove the file extension. 
  temp_tibble <- tibble(dataset=sub('\\.txt$', '', f_name),
                        text=(readLines(file_path)[sample_of_lines]))
  en_US_tibble <- rbind(en_US_tibble,temp_tibble)

} 

# Freeing memory by suppressing unnecessary variables
rm(temp_tibble)


```


## Tokenization

Tokenization will be done by using the unnest_tokens function of the tidytext package. 
In order to compare word frequencies between the 3 datasets (blogs, news, twitter), a column will be kept to keep track of the source file.

```{r}
en_US_token <- en_US_tibble %>% unnest_tokens(word,text)
# Defining  the frequency of words for each file
en_US_token_by_file <- en_US_token %>% group_by(dataset) %>% count(word) %>% mutate(freq_word = n/sum(n)) %>% arrange(dataset,desc(freq_word))
# Switching the dataset column (source file) to factor
en_US_token_by_file$dataset <- as.factor(en_US_token_by_file$dataset)

```

Based on my readings, a common usage in text mining, is to remove "stop words" from the datasets.  
As the purpose of this project is to predict from a first word what could be the next 2 or 3 words, it seems essential to keep those stop words into the dataset as they appear to be necessary to create explicit transitions with other words.  







Turning the text to a tibble

```{r}
twitter_us_df <- tibble(text=raw_text_list)
```

```{r}
twitter_us_df %>%  
  unnest_tokens(word,text) %>%
  count(word) %>%
  arrange(desc(n))
  
```

A common practice there would be to remove the stop words. For the purpose of our application, those words will be kept.  









prints the maximum length of a line of text from the string vector  

```{r}
max(nchar(raw_text))
```

For the twitter dataset = 140 (normal)  
For the news dataset = 11384  
For the blog 40833.  

Finding the number of occurences of words love an hate in the Twitter dataset.  

```{r}
nb_love <- sum(str_count(raw_text, "love"))
nb_have <- sum(str_count(raw_text, "hate"))
```

Finding the occurence of Biostats

```{r}
raw_text[str_detect(raw_text, "biostats")] 
```

A computer once beat me at chess, but it was no match for me at kickboxing

```{r}
raw_text[str_detect(raw_text, "A computer once beat me at chess, but it was no match for me at kickboxing")] 
```


## Basic requirements

```{r}
raw_text_df %>%  
  unnest_tokens(word,text) %>%
  count(word) %>%
  arrange(desc(n))
  
```

## Sampling

Due to the large size of the datasets, they are extremley painful to manipulate.  
EDA will be built on a subset (10%) of randomly sampled datasets.  

```{r}
set.seed(05091810)
```

```{r}
# Create a sample of 50 numbers which are incremented by 1.
x <- seq(0,50,by = 1)

sample(x,size=3, replace =F)
```


